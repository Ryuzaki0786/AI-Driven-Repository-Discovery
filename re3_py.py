# -*- coding: utf-8 -*-
"""Re3.py

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1tX-GmLYeXtOTT_HPfjEXbVPAuzE_8K2X
"""



import requests
from IPython.display import display
import pandas as pd
import xml.etree.ElementTree as ET
from IPython.display import display, HTML
from tqdm.notebook import tqdm

def fetch_re3data_repo_data(target_repo_count=70):
    """
    Fetches the list of repositories from re3data.org API and detailed metadata.
    """
    url = 'https://www.re3data.org/api/v1/repositories?size=100' # Fetch up to 100 repositories
    base_url = 'https://www.re3data.org'

    try:
        response = requests.get(url)
        response.raise_for_status()

        # Parse the XML response
        root = ET.fromstring(response.content)

        # Create a list of dictionaries for the DataFrame
        repo_list = []

        print("Fetching list of Re3data.org Repositories...")

        for repo in root.findall('.//repository')[:target_repo_count]: # Process all found repositories up to size limit
            relative_url = repo.find('link').get('href') if repo.find('link') is not None else None
            full_url = base_url + relative_url if relative_url else None
            repo_name = repo.find('name').text if repo.find('name') is not None else None

            if full_url and repo_name: # Only add if both URL and Name are present
                 repo_list.append({
                    "Name": repo_name,
                    "URL": full_url,

                    "Description": None,
                    "Subjects": None
                 })


        # Create a pandas DataFrame and display it
        print(f"Found {len(repo_list)} repositories.")
        df = pd.DataFrame(repo_list)

        print("\n ----- Fetching Metadata for Repositories -----")

        final_repo_data = []

        for index,row in tqdm(df.iterrows(),total = len(df),desc = "Fetching Metadata"):
            description,subjects = fetch_repo_details(row["URL"])
            final_repo_data.append({
                "ID":f"R{index+1}",
                "Name": row["Name"],
                "URL": row["URL"],
                "Description": description,
                "Subjects": subjects,
                "Combined_Text": f"{row['Name']} {description if description else ''} {subjects if subjects else ''}".lower().strip()
            })
        final_df = pd.DataFrame(final_repo_data)


        print(f"\nSuccessfully fetched detailed data for {len(final_df)} repositories.")
        return final_df

    except requests.exceptions.RequestException as e:
        print(f"Error fetching list data: {e}")
        return None
    except ET.ParseError as e:
        print(f"Error parsing list XML: {e}")
        return None
    except Exception as e:
        print(f"An unexpected error occurred while fetching list: {e}")
        return None
def fetch_repo_details(repo_url):
  """
  Fetches details of a single repository from its URL.
  """
  try:
    response = requests.get(repo_url);
    response.raise_for_status()

    root = ET.fromstring(response.content)

    ns = {'r3d': 'http://www.re3data.org/schema/2-2'}

    description_element = root.find('./r3d:repository/r3d:description',ns)
    description = description_element.text if description_element is not None else None

    subjects =[]

    for subject_element in root.findall('./r3d:repository/r3d:subjects/r3d:subject',ns):
        if subject_element.text:
            subjects.append(subject_element.text.strip());

    subjects_str = ', '.join(subjects)

    return description, subjects_str

  except requests.exceptions.RequestException as e:
    print(f"Error fetching details for {repo_url}: {e}")
    return None, None
  except ET.ParseError as e:
    print(f"Error parsing details XML for {repo_url}: {e}")
    return None, None
  except Exception as e:
    print(f"An unexpected error occurred while fetching details for {repo_url}: {e}")
    return None, None

def load_data():
    """
    Loads and processes the data from re3data.org.
    """
    df = fetch_re3data_repo_data()
    return df

# --- Main execution block ---
if __name__ == "__main__":
    df = load_data()
    if df is not None and not df.empty:
        # Step 2: Check repository data (Optional, can be called separately if needed)
        # checked_df = check_repository_data(df)

        # Step 3: Proceed to model training if data is available
        print("\nReady for Phase 3: Model Training (TF-IDF and Cosine Similarity).")

def check_repository_data(df):
    """
    Checks for broken links and validates data fields for each repository.
    """
    print("\nChecking repository data...")
    checked_repos = []

    if df is not None:
        # Limit to the first 5 repositories as requested
        for index, row in df.head(70).iterrows():
            repo_name = row["Name"]
            repo_url = row["URL"]
            description = row["Description"]
            subjects = row["Subjects"]

            is_valid = True
            status_code = None

            # Check for broken links
            try:
                response = requests.head(repo_url, allow_redirects=True, timeout=10)
                status_code = response.status_code
                if status_code != 200:
                    print(f"Warning: Link for '{repo_name}' returned status code {status_code}")
                    is_valid = False
            except requests.exceptions.RequestException as e:
                print(f"Error checking link for '{repo_name}': {e}")
                is_valid = False

            # Validate data fields (Name and URL are already checked during fetching)
            if not description:
                # print(f"Warning: No description for '{repo_name}'")
                pass # Descriptions are not mandatory in the fetched list, so this is not a critical warning
            if not subjects:
                # print(f"Warning: No subjects for '{repo_name}'")
                pass # Subjects are not mandatory in the fetched list, so this is not a critical warning

            checked_repos.append({
                "Name": repo_name,
                "URL": repo_url,
                "Status Code": status_code,
                "Is Valid": is_valid
            })

        checked_df = pd.DataFrame(checked_repos)
        display(checked_df.head())
        return checked_df

    else:
        print("No data to check.")
        return None

# --- Main execution block ---
if __name__ == "__main__":
    # Step 1: Fetch the list of repositories and get the DataFrame
    df = fetch_re3data_repo_data()

    # Step 2: Check repository data
    if df is not None:
        checked_df = check_repository_data(df)

import pandas as pd
from sklearn.model_selection import train_test_split


corpus_df , test_df = train_test_split(
    df,
    test_size = 10,
    random_state = 42,
    shuffle = True
)
corpus_df = corpus_df.reset_index(drop = True)
test_df =  test_df.reset_index(drop = True)

print(f"Corpus Size (Training): {len(corpus_df)}")
print(f"Test Size (Testing): {len(test_df)}")

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

# 1. Initialize and Fit the Vectorizer ON THE CORPUS
#The model learns the vocabulary and weights only 35 repositories
tfidf = TfidfVectorizer(stop_words = 'english')
corpus_matrix = tfidf.fit_transform(corpus_df['Combined_Text'])

# 2. Calculate the Similarity Matrix (35x35)
# This matrix shows similarity between repositories *within* the corpus
cosine_sim_corpus = cosine_similarity(corpus_matrix, corpus_matrix)

test_matrix = tfidf.transform(test_df['Combined_Text'])


test_vs_corpus_sim = cosine_similarity(test_matrix, corpus_matrix)

def validate_recommendations(test_repo_row_index,test_vs_corpus_sim = test_vs_corpus_sim):

  sim_scores = list(enumerate(test_vs_corpus_sim[test_repo_row_index]))

  sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)

  top_indices = [i[0] for i in sim_scores[:3]]

  recommended_names =  corpus_df['Name'].iloc[top_indices].tolist()
  print("--- Test Result ---")
  print(f"TEST REPO: {test_df['Name'].iloc[test_repo_row_index]}")
  print("TOP 3 RECOMMENDATIONS (from Corpus):")
  for name in recommended_names:
      print(f"- {name}")
  print("\n" + "="*40)


# --- Run the Validation ---
# Test the model on the very first held-out repository (index 0)
validate_recommendations(0)

# Test the model on another held-out repository (index 1)
validate_recommendations(1)

validate_recommendations(2)

from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
import matplotlib.pyplot as plt
import seaborn as sns

# Use PCA for initial dimensionality reduction (often helpful before t-SNE)
pca = PCA(n_components=50, random_state=42) # Reduce to 50 components
pca_result = pca.fit_transform(corpus_matrix)

# Use t-SNE to further reduce to 2 dimensions for visualization
tsne = TSNE(n_components=2, random_state=42, perplexity=min(30, len(pca_result) - 1), n_iter=300)
tsne_result = tsne.fit_transform(pca_result)

# Create a DataFrame for plotting
tsne_df = pd.DataFrame(data=tsne_result, columns=['Component 1', 'Component 2'])
tsne_df['Repository Name'] = corpus_df['Name']

# Plot the results
plt.figure(figsize=(12, 10)) # Increased figure size
sns.scatterplot(
    x="Component 1", y="Component 2",
    hue="Repository Name",
    palette=sns.color_palette("tab20", len(tsne_df)), # Using a different color palette
    data=tsne_df,
    legend="full", # Show the full legend
    alpha=0.8,
    s=50 # Increased marker size
)
plt.title('t-SNE visualization of Repository Feature Space', fontsize=16) # Increased title font size
plt.xlabel('Component 1 (Reduced Dimension)', fontsize=12) # Increased label font size
plt.ylabel('Component 2 (Reduced Dimension)', fontsize=12) # Increased label font size
plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.) # Position the legend outside the plot
plt.grid(True, linestyle='--', alpha=0.6) # Add a grid
plt.tight_layout() # Adjust layout to prevent labels overlapping
plt.show()

"""**Explanation of the t-SNE plot:**

Each point in the scatter plot represents a different repository. The t-SNE algorithm attempts to place repositories with similar text content (based on their TF-IDF vectors) closer together in the 2D space, while placing dissimilar repositories further apart. The different colors distinguish the individual repositories as listed in the legend to the right of the plot.
"""

# Commented out IPython magic to ensure Python compatibility.
# %pip install Flask

import requests
from flask import Flask, request, jsonify

# from flask import Flask, request, jsonify
# import joblib
# from sklearn.metrics.pairwise import cosine_similarity
# import pandas as pd

# #File Paths
# MODEL_Pth = 'tfidf_vectorizer.joblib'
# MATRIX_Pth = 'cosine_sim_matrix.joblib'
# DATA_Pth = 'corpus_df.joblib'

# try:
#   tfidf = joblib.load(MODEL_Pth)
#   corpus_matrix = joblib.load(MATRIX_Pth)

#   corpus_df = joblib.load(DATA_Pth)
#   print("Model Components loaded scucessfully!")

# except Exception as e:
#   print(f"Error loading model components: {e}")

# def get_recommendations(input_text,top_n=3):

#   input_matrix = tfidf.transform([input_text])

#   similarity_scores = cosine_similarity(input_matrix,corpus_matrix)

#   sim_scores = list(enumerate(similarity_scores[0]))

#   sim_scores = sorted(sim_scores,key=lambda x:x[1],reverse=True)

#   top_n_scores = sim_scores[:top_n]
#   top_indices = [i[0] for i in top_n_scores]

#   recommendations = []
#   for index,score in top_n_scores:
#     repo_data = corpus_df.iloc[index]
#     recommendations.append({
#         "Name": repo_data["Name"],
#         "URL": repo_data["URL"],
#         "Similarity Score": float(score)
#     })

#   return recommendations

# from flask import Flask, request, jsonify

# app = Flask(__name__)

# @app.route('/recommend',methods = ['POST'])
# def home():
#     data = request.get_json();

#     input_text = data.get('text')

#     if not input_text:
#       return jsonify({
#           "error": "No Text Provided"}),400
#     recommendations = get_recommendations(input_text)
#     return jsonify(recommendations)

#     return "Repository Recommendation API is running!"


# if __name__ == '__main__':
#     app.run(debug=True)

"""1.  **`from flask import Flask, request, jsonify`**: This line imports the necessary components from the `flask` library.
    *   `Flask`: This is the main class we use to create our application.
    *   `request`: This object will allow us to access incoming request data (like the text we want to get recommendations for).
    *   `jsonify`: This function helps us return data in a standard format called JSON (JavaScript Object Notation), which is commonly used for APIs.

2.  **`app = Flask(__name__)`**: This creates an instance of the Flask application. `__name__` is a special Python variable that gets the name of the current module. Flask uses this to figure out where to look for resources like templates and static files.

3.  **`@app.route('/')`**: This is a *decorator*. Decorators are a powerful feature in Python that modify the behavior of a function. In this case, `@app.route('/')` tells Flask that when a user visits the root URL of our application (`/`), the code inside the `home()` function should be executed.

4.  **`def home():`**: This defines a function named `home`.

5.  **`return "Repository Recommendation API is running!"`**: This line tells the `home()` function to return the string "Repository Recommendation API is running!". When someone visits the root URL, this is what they will see in their browser.

6.  **`if __name__ == '__main__':`**: This is a standard Python construct. It means the code inside this block will only run when the script is executed directly (not when it's imported as a module into another script).

7.  **`app.run(debug=True)`**: This starts the Flask development server.
    *   `app.run()`: This is the command to run the application.
    *   `debug=True`: This enables debug mode. In debug mode, the server will automatically reload if you make changes to the code, and it will provide helpful error messages in your browser if something goes wrong. It's great for development but should be turned off in production.

Go ahead and run this cell. You should see output indicating that the Flask development server is running. It will provide a local URL (usually something like `http://127.0.0.1:5000/`). You can click on this URL to see the message "Repository Recommendation API is running!".

# Explaining the Re3data Repository Recommendation Code

This document provides a detailed explanation of the Python code used to fetch data from re3data.org, process it using TF-IDF, and then use cosine similarity to recommend repositories.

## 1. Data Fetching from re3data.org

The code begins by fetching data about research data repositories from the re3data.org API.

### `fetch_re3data_repo_data()` function

This function is responsible for retrieving the initial list of repositories and then fetching detailed information for each.

- **API Endpoint:** The code uses the URL `https://www.re3data.org/api/v1/repositories?size=100` to access the re3data.org API. The `size=100` parameter limits the initial response to 100 repositories.
- **HTTP Request:** The `requests.get(url)` call sends an HTTP GET request to the API endpoint.
- **Error Handling:** `response.raise_for_status()` checks if the request was successful. If not, it raises an HTTPError.
- **XML Parsing:** The response content, which is in XML format, is parsed using `xml.etree.ElementTree as ET`. The root element of the XML is obtained.
- **Extracting Repository Information:** The code iterates through `<repository>` elements in the XML. For each repository, it extracts:
    - **Name:** The text content of the `<name>` element.
    - **URL:** The `href` attribute of the `<link>` element. This URL is used to fetch detailed information later.
- **Creating Initial DataFrame:** The extracted names and URLs are stored in a list of dictionaries, which is then converted into a pandas DataFrame.
- **Fetching Detailed Data:** The code then iterates through the rows of the initial DataFrame. For each repository URL, it calls the `fetch_repo_details()` function to get more information.
- **Combining Data and Creating Final DataFrame:** The detailed information (description and subjects) fetched by `fetch_repo_details()` is added to the data. A "Combined_Text" column is created by concatenating the name, description, and subjects. This combined text is crucial for the TF-IDF vectorization. A final DataFrame containing "ID", "Name", "URL", "Description", "Subjects", and "Combined_Text" is created and returned.
- **Additional Error Handling:** `try...except` blocks are used to catch potential `requests.exceptions.RequestException` (for HTTP errors), `ET.ParseError` (for XML parsing errors), and general `Exception` during both the initial fetch and the detailed fetch.

### `fetch_repo_details(repo_url)` function

This function fetches detailed information for a single repository given its API URL.

- **HTTP Request:** It sends an HTTP GET request to the provided `repo_url`.
- **Error Handling:** `response.raise_for_status()` checks for HTTP errors.
- **XML Parsing:** The XML response for the specific repository is parsed.
- **Extracting Details:**
    - **Description:** It finds the `<r3d:description>` element within the `<r3d:repository>` element using an XML namespace (`r3d`). The text content of this element is extracted as the description.
    - **Subjects:** It finds all `<r3d:subject>` elements within `<r3d:subjects>`. The text content of each subject element is extracted and added to a list.
- **Returning Details:** The extracted description and a comma-separated string of subjects are returned.
- **Error Handling:** Similar `try...except` blocks are used as in `fetch_re3data_repo_data()` to handle potential errors during the details fetching process.

## 2. Data Preparation for Modeling

Before applying TF-IDF and cosine similarity, the data is prepared.

### Splitting Data into Training and Testing Sets

- **`train_test_split(df, test_size=10, random_state=42, shuffle=True)`:** This function from `sklearn.model_selection` splits the main DataFrame (`df`) into two subsets:
    - `corpus_df`: This will be used as the "corpus" for the TF-IDF vectorizer. The vectorizer will learn the vocabulary and IDF scores from this data.
    - `test_df`: This will be used to test the recommendation system. We will use the models trained on the corpus to find similar repositories for the test repositories.
- **`test_size=10`:**  Specifies that 10 repositories will be put into the test set.
- **`random_state=42`:** Ensures that the split is the same every time the code is run, making the results reproducible.
- **`shuffle=True`:** Randomly shuffles the data before splitting.
- **`reset_index(drop=True)`:** Resets the index of both the `corpus_df` and `test_df` DataFrames, dropping the original index. This is important for consistent indexing in subsequent steps.

## 3. TF-IDF Vectorization

TF-IDF (Term Frequency-Inverse Document Frequency) is a numerical statistic that reflects how important a word is to a document in a collection or corpus.

- **Term Frequency (TF):** Measures how frequently a term appears in a document.
- **Inverse Document Frequency (IDF):** Measures how important a term is across the whole corpus. Rare terms have a higher IDF score.

The TF-IDF score is the product of TF and IDF. It gives more weight to terms that are frequent in a specific document but rare in the overall corpus.

### Applying TF-IDF

- **`TfidfVectorizer(stop_words='english')`:** An instance of `TfidfVectorizer` is created.
    - **`stop_words='english'`:** This argument tells the vectorizer to remove common English stop words (like "the", "a", "is") from the text before calculating TF-IDF scores. This helps focus on more meaningful terms.
- **`corpus_matrix = tfidf.fit_transform(corpus_df['Combined_Text'])`:**
    - **`fit`:** The vectorizer learns the vocabulary of the `Combined_Text` column in the `corpus_df` (the training data). It also calculates the IDF scores for each term based on its frequency across all documents in the corpus.
    - **`transform`:** It converts the `Combined_Text` of each repository in the corpus into a TF-IDF vector. Each vector represents a repository as a sequence of numbers, where each number is the TF-IDF score of a term in the vocabulary. The result is a sparse matrix (`corpus_matrix`) where rows represent repositories and columns represent terms.

## 4. Cosine Similarity

Cosine similarity is a metric used to measure how similar two non-zero vectors are. It calculates the cosine of the angle between two vectors. A cosine similarity of 1 means the vectors are identical in direction (most similar), while a similarity of 0 means they are orthogonal (no similarity).

In this context, cosine similarity is used to find how similar the TF-IDF vector of one repository is to the TF-IDF vectors of other repositories. A higher cosine similarity score indicates a greater similarity between the content (name, description, subjects) of the repositories.

### Calculating Cosine Similarity

- **`cosine_sim_corpus = cosine_similarity(corpus_matrix, corpus_matrix)`:** This calculates the cosine similarity between all pairs of repositories within the corpus. The result is a square matrix (`cosine_sim_corpus`) where the entry at row `i` and column `j` represents the cosine similarity between the i-th and j-th repositories in the corpus.

## 5. Testing and Validation

The code then prepares the test data and uses the trained TF-IDF vectorizer and cosine similarity to recommend repositories from the corpus for the test repositories.

- **`test_matrix = tfidf.transform(test_df['Combined_Text'])`:** The `transform` method of the *already fitted* `tfidf` vectorizer is used to convert the `Combined_Text` of the test repositories into TF-IDF vectors. It's important to use `transform` here, not `fit_transform`, because the vectorizer should use the vocabulary and IDF scores learned from the training corpus.
- **`test_vs_corpus_sim = cosine_similarity(test_matrix, corpus_matrix)`:** This calculates the cosine similarity between the TF-IDF vectors of the test repositories and the TF-IDF vectors of the corpus repositories. The result is a matrix where each row corresponds to a test repository and each column corresponds to a corpus repository. The entry at row `i` and column `j` is the cosine similarity between the i-th test repository and the j-th corpus repository.

### `validate_recommendations(test_repo_row_index, test_vs_corpus_sim)` function

This function takes the index of a test repository and the `test_vs_corpus_sim` matrix to find and display the top recommended repositories from the corpus.

- **`sim_scores = list(enumerate(test_vs_corpus_sim[test_repo_row_index]))`:** This extracts the row from the `test_vs_corpus_sim` matrix that corresponds to the given `test_repo_row_index`. This row contains the similarity scores between the test repository and all corpus repositories. `enumerate` is used to pair each score with its original index in the corpus.
- **`sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)`:** The list of (index, score) tuples is sorted in descending order based on the similarity score (`x[1]`).
- **`top_indices = [i[0] for i in sim_scores[:3]]`:** The indices of the top 3 most similar corpus repositories are extracted from the sorted list.
- **`recommended_names = corpus_df['Name'].iloc[top_indices].tolist()`:** The names of the recommended repositories are retrieved from the `corpus_df` using the extracted top indices.
- **Printing Results:** The function then prints the name of the test repository and the names of the top 3 recommended repositories from the corpus.

## Conclusion

This code demonstrates a basic content-based recommendation system for research data repositories. By fetching data from re3data.org, vectorizing the repository descriptions and subjects using TF-IDF, and then calculating cosine similarity, the system can identify repositories that are similar in content. This approach can be extended and refined for more sophisticated recommendation engines.

# Task
Create a Flask API for a repository recommendation model. The API should have an endpoint that accepts repository data as input, uses the pre-trained TF-IDF model and corpus to find similar repositories, and returns the recommendations.

## Refactor the code

### Subtask:
Refactor the existing code for fetching data, training the TF-IDF model, and calculating cosine similarity into reusable functions or classes.

**Reasoning**:
Refactor the data loading and initial processing into a function `load_data`.
"""